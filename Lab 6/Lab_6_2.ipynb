{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "Lab_6_2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "source": [
        "from __future__ import absolute_import, division, print_function\r\n",
        "\r\n",
        "import tensorflow as tf\r\n",
        "\r\n",
        "import numpy as np"
      ],
      "outputs": [],
      "metadata": {
        "id": "Yv_lJKqQS8Gy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "MNIST data set. MNIST data is a collection of hand-written digits that contains 60,000 examples for training and 10,000 examples for testing. The digits have been size-normalized and centered in a fixed-size image (28x28 pixels) with values from 0 to 255. \n",
        "\n",
        "Next for each image we will:\n",
        "\n",
        "1) converted it to float32\n",
        "\n",
        "2) normalized to [0, 1]\n",
        "\n",
        "3) flattened to a 1-D array of 784 features (28*28).\n",
        "\n"
      ],
      "metadata": {
        "id": "Fov3qx-jTSyf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 2: Loading and Preparing the MNIST Data Set"
      ],
      "metadata": {
        "id": "AL23LF3zTWhd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "source": [
        "from tensorflow.keras.datasets import mnist\r\n",
        "\r\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\r\n",
        "\r\n",
        "# Convert to float32.\r\n",
        "x_train, x_test = np.array(x_train, np.float32), np.array(x_test, np.float32)\r\n",
        "\r\n",
        "\r\n",
        "# Flatten images to 1-D vector of 784 features (28*28).\r\n",
        "num_features=784\r\n",
        "x_train, x_test = x_train.reshape([-1, num_features]), x_test.reshape([-1, num_features])\r\n",
        "\r\n",
        "\r\n",
        "# Normalize images value from [0, 255] to [0, 1].\r\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YsQ40c2uTV6c",
        "outputId": "d5a46836-a9d8-42a0-bc83-16466af5b4c9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 3: Setting Up Hyperparameters and Data Set Parameters\n",
        "\n",
        "Initialize the model parameters. \n",
        "\n",
        "num_classes denotes the number of outputs, which is 10, as we have digits from 0 to 9 in the data set. \n",
        "\n",
        "num_features defines the number of input parameters, and we store 784 since each image contains 784 pixels."
      ],
      "metadata": {
        "id": "PdAf-R2QTxWq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "source": [
        "# MNIST dataset parameters.\r\n",
        "\r\n",
        "num_classes = 10 # 0 to 9 digits\r\n",
        "\r\n",
        "num_features = 784 # 28*28\r\n",
        "\r\n",
        "# Training parameters.\r\n",
        "\r\n",
        "learning_rate = 0.01\r\n",
        "\r\n",
        "training_steps = 1000\r\n",
        "\r\n",
        "batch_size = 256\r\n",
        "\r\n",
        "display_step = 50"
      ],
      "outputs": [],
      "metadata": {
        "id": "lQ2LYiXkTNsU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 4: Shuffling and Batching the Data\n",
        "\n",
        "We need to shuffle and batch the data before we start the actual training to avoid the model from getting biased by the data. This will allow our data to be more random and helps our model to gain higher accuracies with the test data.\n",
        "\n",
        "With the help of tf.data.Dataset.from_tensor_slices, we can get the slices of an array in the form of objects. \n",
        "\n",
        "The function shuffle(5000) randomizes the order of the data setâ€™s examples. \n",
        "\n",
        "Here, 5000 denotes the variable shuffle_buffer, which tells the model to pick a sample randomly from 1 to 5000 samples. \n",
        "\n",
        "After that, there are only 4999 samples left in the buffer, so the sample 5001 gets added to the buffer. "
      ],
      "metadata": {
        "id": "ghG3hKYXT0c9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "source": [
        "# Use tf.data API to shuffle and batch data.\r\n",
        "train_data=tf.data.Dataset.from_tensor_slices((x_train,y_train))\r\n",
        "\r\n",
        "train_data=train_data.repeat().shuffle(5000).batch(batch_size).prefetch(1)"
      ],
      "outputs": [],
      "metadata": {
        "id": "nfpwSWugTs0J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 5: Initializing Weights and Biases\n",
        "\n",
        "We now initialize the weights vector and bias vector with ones and zeros."
      ],
      "metadata": {
        "id": "85GK6CKmUAye"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "source": [
        "# Weight of shape [784, 10], the 28*28 image features, and a total number of classes.\r\n",
        "W = tf.Variable(tf.ones([num_features, num_classes]), name=\"weight\")\r\n",
        "\r\n",
        "# Bias of shape [10], the total number of classes.\r\n",
        "b = tf.Variable(tf.zeros([num_classes]), name=\"bias\")\r\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "sL2gAFz9T-fK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 6: Defining Logistic Regression and Cost Function\n",
        "\n",
        "We define the logistic_regression function below, which converts the inputs into a probability distribution proportional to the exponents of the inputs using the softmax function. The softmax function, which is implemented using the function tf.nn.softmax, also makes sure that the sum of all the inputs equals one."
      ],
      "metadata": {
        "id": "bM2mMzZ_UGNC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "source": [
        "# Logistic regression (Wx + b).\r\n",
        "\r\n",
        "def logistic_regression(x):\r\n",
        "\r\n",
        "    # Apply softmax to normalize the logits to a probability distribution.\r\n",
        "    return tf.nn.softmax(tf.matmul(x, W) + b)\r\n",
        "    \r\n",
        "\r\n",
        "# Cross-Entropy loss function.\r\n",
        "\r\n",
        "def cross_entropy(y_pred, y_true):\r\n",
        "\r\n",
        "    # Encode label to a one hot vector.\r\n",
        "    y_true = tf.one_hot(y_true, depth=num_classes)    \r\n",
        "\r\n",
        "    # Clip prediction values to avoid log(0) error.\r\n",
        "    y_pred = tf.clip_by_value(y_pred, 1e-9, 1.)    \r\n",
        "\r\n",
        "    # Compute cross-entropy.\r\n",
        "    return tf.reduce_mean(-tf.reduce_sum(y_true * tf.math.log(y_pred)))"
      ],
      "outputs": [],
      "metadata": {
        "id": "R73nGqh9T-S8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 7: Defining Optimizers and Accuracy Metrics\n",
        "When we compute the output, it gives us the probability of the given data to fit a particular class of output. \n",
        "\n",
        "We consider the correct prediction as to the class having the highest probability. \n",
        "\n",
        "We compute this using the function tf.argmax. \n",
        "\n",
        "We also define the stochastic gradient descent as the optimizer from several optimizers present in TensorFlow. We do this using the function tf.optimizers.SGD. \n",
        "\n",
        "This function takes in the learning rate as its input, which defines how fast the model should reach its minimum loss or gain the highest accuracy."
      ],
      "metadata": {
        "id": "SfWiF9rmUMAS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "source": [
        "# Accuracy metric.\r\n",
        "\r\n",
        "def accuracy(y_pred, y_true):\r\n",
        "\r\n",
        "  # Predicted class is the index of the highest score in prediction vector (i.e. argmax).\r\n",
        "  correct_prediction = tf.equal(tf.argmax(y_pred, 1), tf.cast(y_true, tf.int64))\r\n",
        "  return tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\r\n",
        "\r\n",
        "optimizer = tf.optimizers.SGD(learning_rate)"
      ],
      "outputs": [],
      "metadata": {
        "id": "MZGYfEppUNUR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 8: Optimization Process and Updating Weights and Biases\n",
        "Now we define the run_optimization() method where we update the weights of our model. We calculate the predictions using the logistic_regression(x) method by taking the inputs and find out the loss generated by comparing the predicted value and the original value present in the data set. Next, we compute the gradients using and update the weights of the model with our stochastic gradient descent optimizer."
      ],
      "metadata": {
        "id": "OZl-oNlzUS_y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "source": [
        "# Optimization process. \r\n",
        "def run_optimization(x, y):\r\n",
        "  # Wrap computation inside a GradientTape for automatic differentiation.\r\n",
        "  with tf.GradientTape() as g:\r\n",
        "      pred = logistic_regression(x)\r\n",
        "      loss = cross_entropy(pred, y)\r\n",
        "\r\n",
        "  # Compute gradients.\r\n",
        "  gradients = g.gradient(loss, [W, b])\r\n",
        "\r\n",
        "  # Update W and b following gradients.\r\n",
        "  optimizer.apply_gradients(zip(gradients, [W, b]))"
      ],
      "outputs": [],
      "metadata": {
        "id": "Vym_wu1_UQy1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 9: The Training Loop"
      ],
      "metadata": {
        "id": "xUMJnB4oUe7K"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "source": [
        "# Run training for the given number of steps.\r\n",
        "\r\n",
        "for step, (batch_x, batch_y) in enumerate(train_data.take(training_steps), 1):\r\n",
        "\r\n",
        "    # Run the optimization to update W and b values.\r\n",
        "    run_optimization(batch_x, batch_y)\r\n",
        "\r\n",
        "    if step % display_step == 0:\r\n",
        "        pred = logistic_regression(batch_x)\r\n",
        "        loss = cross_entropy(pred, batch_y)\r\n",
        "        acc = accuracy(pred, batch_y)\r\n",
        "        print(\"step: %i, loss: %f, accuracy: %f\" % (step, loss, acc))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step: 50, loss: 105.539932, accuracy: 0.890625\n",
            "step: 100, loss: 60.250996, accuracy: 0.925781\n",
            "step: 150, loss: 79.272217, accuracy: 0.914062\n",
            "step: 200, loss: 87.701462, accuracy: 0.917969\n",
            "step: 250, loss: 101.452927, accuracy: 0.894531\n",
            "step: 300, loss: 76.463478, accuracy: 0.917969\n",
            "step: 350, loss: 59.819138, accuracy: 0.957031\n",
            "step: 400, loss: 67.821899, accuracy: 0.945312\n",
            "step: 450, loss: 42.849136, accuracy: 0.960938\n",
            "step: 500, loss: 92.421310, accuracy: 0.914062\n",
            "step: 550, loss: 192.526062, accuracy: 0.859375\n",
            "step: 600, loss: 85.016479, accuracy: 0.910156\n",
            "step: 650, loss: 82.445862, accuracy: 0.933594\n",
            "step: 700, loss: 50.144257, accuracy: 0.949219\n",
            "step: 750, loss: 102.714340, accuracy: 0.882812\n",
            "step: 800, loss: 84.259445, accuracy: 0.933594\n",
            "step: 850, loss: 71.858780, accuracy: 0.929688\n",
            "step: 900, loss: 75.666367, accuracy: 0.917969\n",
            "step: 950, loss: 63.687168, accuracy: 0.925781\n",
            "step: 1000, loss: 135.350754, accuracy: 0.886719\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PLPbvU2ZUcHp",
        "outputId": "2498e64b-39cb-4fdb-f8a1-04d48595dde4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 10: Testing Model Accuracy Using the Test Data\n",
        "\n",
        "Finally, we check the model accuracy by sending the test data set into our model and compute the accuracy using the accuracy function that we defined earlier."
      ],
      "metadata": {
        "id": "Wjr-WgeYUtCZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "source": [
        "# Test model on validation set.\r\n",
        "pred = logistic_regression(x_test)\r\n",
        "\r\n",
        "print(\"Test Accuracy: %f\" % accuracy(pred, y_test))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.856000\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R0OuQF0eUtza",
        "outputId": "28cede40-1812-4f22-ed0b-ac58ebd9406c"
      }
    }
  ]
}