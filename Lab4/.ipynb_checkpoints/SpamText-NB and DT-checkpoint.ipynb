{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Using cached https://files.pythonhosted.org/packages/de/f0/96fb2e0412ae9692dbf400e5b04432885f677ad6241c088ccc5fe7724d69/tensorflow-1.14.0-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Collecting numpy<2.0,>=1.14.5 (from tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/45/b2/6c7545bb7a38754d63048c7696804a0d947328125d81bf12beaa692c3ae3/numpy-1.19.5-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Collecting gast>=0.2.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/ad/b0/a6c8bc1e77409b63755bb0788ac4d497d31b72082de5945c0122259c533e/gast-0.5.2-py3-none-any.whl\n",
      "Collecting wrapt>=1.11.1 (from tensorflow)\n",
      "Collecting keras-preprocessing>=1.0.5 (from tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/79/4c/7c3275a01e12ef9368a892926ab932b33bb13d55794881e3573482b378a7/Keras_Preprocessing-1.1.2-py2.py3-none-any.whl\n",
      "Collecting google-pasta>=0.1.6 (from tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/a3/de/c648ef6835192e6e2cc03f40b19eeda4382c49b5bafb43d88b931c4c74ac/google_pasta-0.2.0-py3-none-any.whl\n",
      "Collecting termcolor>=1.1.0 (from tensorflow)\n",
      "Collecting grpcio>=1.8.6 (from tensorflow)\n",
      "Collecting absl-py>=0.7.0 (from tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/23/47/835652c7e19530973c73c65e652fc53bd05725d5a7cf9bb8706777869c1e/absl_py-0.13.0-py3-none-any.whl\n",
      "Collecting six>=1.10.0 (from tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/d9/5a/e7c31adbe875f2abbb91bd84cf2dc52d792b5a01506781dbcf25c91daf11/six-1.16.0-py2.py3-none-any.whl\n",
      "Collecting wheel>=0.26 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/04/80/cad93b40262f5d09f6de82adbee452fd43cdff60830b56a74c5930f7e277/wheel-0.37.0-py2.py3-none-any.whl\n",
      "Collecting astor>=0.6.0 (from tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/c3/88/97eef84f48fa04fbd6750e62dcceafba6c63c81b7ac1420856c8dcc0a3f9/astor-0.8.1-py2.py3-none-any.whl\n",
      "Collecting keras-applications>=1.0.6 (from tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl\n",
      "Collecting protobuf>=3.6.1 (from tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/53/4e/e2db88d0bb0bda6a879eea62fddbaf813719ce3770d458bc5580512d9c95/protobuf-3.17.3-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.whl\n",
      "Collecting tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 (from tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/3c/d5/21860a5b11caf0678fbc8319341b0ae21a07156911132e0e71bffed0510d/tensorflow_estimator-1.14.0-py2.py3-none-any.whl\n",
      "Collecting tensorboard<1.15.0,>=1.14.0 (from tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/91/2d/2ed263449a078cd9c8a9ba50ebd50123adf1f8cfbea1492f9084169b89d9/tensorboard-1.14.0-py3-none-any.whl\n",
      "Collecting h5py (from keras-applications>=1.0.6->tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/70/7a/e53e500335afb6b1aade11227cdf107fca54106a1dca5c9d13242a043f3b/h5py-3.1.0-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Collecting markdown>=2.6.8 (from tensorboard<1.15.0,>=1.14.0->tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/6e/33/1ae0f71395e618d6140fbbc9587cc3156591f748226075e0f7d6f9176522/Markdown-3.3.4-py3-none-any.whl\n",
      "Collecting werkzeug>=0.11.15 (from tensorboard<1.15.0,>=1.14.0->tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/bd/24/11c3ea5a7e866bf2d97f0501d0b4b1c9bbeade102bb4b588f0d2919a5212/Werkzeug-2.0.1-py3-none-any.whl\n",
      "Collecting setuptools>=41.0.0 (from tensorboard<1.15.0,>=1.14.0->tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/bd/25/5bdf7f1adeebd4e3fa76b2e2f045ae53ee208e40a4231ad0f0c3007e4353/setuptools-57.4.0-py3-none-any.whl\n",
      "Collecting cached-property; python_version < \"3.8\" (from h5py->keras-applications>=1.0.6->tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/48/19/f2090f7dad41e225c7f2326e4cfe6fff49e57dedb5b53636c9551f86b069/cached_property-1.5.2-py2.py3-none-any.whl\n",
      "Collecting importlib-metadata; python_version < \"3.8\" (from markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/07/76/c4674c460f5ff4b5f7a962214e46295e20504dfde9fcba78fd728dfe2ac9/importlib_metadata-4.6.3-py3-none-any.whl\n",
      "Collecting dataclasses; python_version < \"3.7\" (from werkzeug>=0.11.15->tensorboard<1.15.0,>=1.14.0->tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/fe/ca/75fac5856ab5cfa51bbbcefa250182e50441074fdc3f803f6e76451fab43/dataclasses-0.8-py3-none-any.whl\n",
      "Collecting zipp>=0.5 (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/92/d9/89f433969fb8dc5b9cbdd4b4deb587720ec1aeb59a020cf15002b9593eef/zipp-3.5.0-py3-none-any.whl\n",
      "Collecting typing-extensions>=3.6.4; python_version < \"3.8\" (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/2e/35/6c4fff5ab443b57116cb1aad46421fb719bed2825664e8fe77d66d99bcbc/typing_extensions-3.10.0.0-py3-none-any.whl\n",
      "Installing collected packages: numpy, gast, wrapt, six, keras-preprocessing, google-pasta, termcolor, grpcio, absl-py, wheel, astor, cached-property, h5py, keras-applications, protobuf, tensorflow-estimator, zipp, typing-extensions, importlib-metadata, markdown, dataclasses, werkzeug, setuptools, tensorboard, tensorflow\n",
      "Successfully installed absl-py-0.13.0 astor-0.8.1 cached-property-1.5.2 dataclasses-0.8 gast-0.5.2 google-pasta-0.2.0 grpcio-1.39.0 h5py-3.1.0 importlib-metadata-4.6.3 keras-applications-1.0.8 keras-preprocessing-1.1.2 markdown-3.3.4 numpy-1.19.5 protobuf-3.17.3 setuptools-57.4.0 six-1.16.0 tensorboard-1.14.0 tensorflow-1.14.0 tensorflow-estimator-1.14.0 termcolor-1.1.0 typing-extensions-3.10.0.0 werkzeug-2.0.1 wheel-0.37.0 wrapt-1.12.1 zipp-3.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zrZevCPJ92HG"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "stopwords = [ \"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\", \"is\", \"it\", \"it's\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \"so\", \"some\", \"such\", \"than\", \"that\", \"that's\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\", \"these\", \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"were\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\", \"which\", \"while\", \"who\", \"who's\", \"whom\", \"why\", \"why's\", \"with\", \"would\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\" ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 503
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2769,
     "status": "ok",
     "timestamp": 1596080419533,
     "user": {
      "displayName": "Prof. Hariom Pandya",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggt3sg6X_951s0boD3SSJvqRng4AQaC3MhTBtGQ9Q=s64",
      "userId": "16159546014304882594"
     },
     "user_tz": -330
    },
    "id": "1rmYBjsyCv3K",
    "outputId": "28ff5c05-155c-4639-f53c-c656d407ff0d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data :\n",
      "        v1                                                 v2\n",
      "0    spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
      "1    spam  FreeMsg Hey there darling it's been 3 week's n...\n",
      "2    spam  WINNER!! As a valued network customer you have...\n",
      "3    spam  Had your mobile 11 months or more? U R entitle...\n",
      "4    spam  SIX chances to win CASH! From 100 to 20,000 po...\n",
      "..    ...                                                ...\n",
      "508  spam  This is the 2nd time we have tried 2 contact u...\n",
      "509   ham              Will �_ b going to esplanade fr home?\n",
      "510   ham  Pity, * was in mood for that. So...any other s...\n",
      "511   ham  The guy did some bitching but I acted like i'd...\n",
      "512   ham                         Rofl. Its true to its name\n",
      "\n",
      "[513 rows x 2 columns]\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 513 entries, 0 to 512\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   v1      513 non-null    object\n",
      " 1   v2      513 non-null    object\n",
      "dtypes: object(2)\n",
      "memory usage: 8.1+ KB\n",
      "\n",
      "Data statistics\n",
      " None\n"
     ]
    }
   ],
   "source": [
    "datasets = pd.read_csv('spam1.csv') \n",
    "print(\"\\nData :\\n\",datasets)\n",
    "print(\"\\nData statistics\\n\",datasets.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qkvmsbgkNRe4"
   },
   "source": [
    "##Analysis\n",
    "\n",
    "To analyze the text data, we have to turn the words into numerical numbers. \n",
    "We have multiple choices to accomplish this step: \n",
    "\n",
    "1) Binary Term Frequency :  count presence(1) or absence(0) for term in document\n",
    "\n",
    "2) Bag of Words Frequency:  captures the frequency of term in document\n",
    "\n",
    "3) Term Frequency: \n",
    "\n",
    "4) TFIDF :\n",
    "\n",
    "in this way, if a term appears frequently in a document, it’s important; if a term appears in many documents, it’s not a unique identifier.\n",
    "\n",
    "Word2Vec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TbhXzh5ONQ5R"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rFDvCoaVbEUV"
   },
   "source": [
    "#Next we use CountVectorizer:\n",
    "\n",
    "More Details and example at:\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9LhzBBgSC3S5"
   },
   "outputs": [],
   "source": [
    "#MINOR CORRECTION IN LAB'S CODE:\n",
    "\n",
    "#BY MISTAKE WE HAVE APPLIED \"FIT_TRANSFORM\" TO TEST DATA ALSO INSTEAD OF ONLY \"TRANSFORM\"\n",
    "#AND WHENEVER WE HAVE CORRECTED IT, THE XTRAIN,XTEST WAS ALREADY MODIFIED\n",
    "#SO, SIMPLE RERUN YOUR TRAIN_TEST_SPLIT CODE AND NEXT IF YOU TRY WITH THE GIVEN CODE, IT WILL WORK\n",
    "\n",
    "#Hope You got the Point !!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e9vB3hKHIEFr"
   },
   "outputs": [],
   "source": [
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NcC9U7_DHw03"
   },
   "source": [
    "**Naive Bayes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7ZAmSYVKIWG5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cYu4_PnbIxfw"
   },
   "source": [
    "**Decision Tree**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oqjGef-FIw7U"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3i5VpsczbgsS"
   },
   "source": [
    "**Optional Exercise:**\n",
    "Try this on full spam.csv file and bigram matching instead of unigram matching "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "SpamText-NB and DT.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
